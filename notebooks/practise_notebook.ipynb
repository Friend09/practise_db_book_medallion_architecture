{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTISE\n",
    "\n",
    "# PySpark Practice Notebook for Data Engineering & ETL on Databricks\n",
    "\n",
    "This notebook is designed for hands-on practice with PySpark, focusing on ETL pipelines and data engineering tasks commonly performed in Databricks environments.\n",
    "\n",
    "## Sections:\n",
    "\n",
    "1. Environment Setup & Installation\n",
    "2. SparkSession Initialization\n",
    "3. Basic DataFrame Operations\n",
    "4. Data Ingestion (CSV, Parquet, JSON)\n",
    "5. Transformations & Actions\n",
    "6. ETL Pipeline Example\n",
    "7. DataFrame Joins & Aggregations\n",
    "8. Writing Data (Parquet, Delta, etc.)\n",
    "9. Useful Tips & Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Installation\n",
    "\n",
    "Ensure you have PySpark installed. If not, run the following cell to install it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark if not already installed\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SparkSession Initialization\n",
    "\n",
    "Create a SparkSession, which is the entry point to using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName('PySparkPractice').getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print('Spark Version:', spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "Create a DataFrame, view schema, and perform simple operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1, 'Alice', 29), (2, 'Bob', 31), (3, 'Cathy', 25)]\n",
    "columns = ['id', 'name', 'age']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# Select columns\n",
    "df.select('name', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Ingestion (CSV, Parquet, JSON)\n",
    "\n",
    "Read data from different file formats into DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading CSV, Parquet, and JSON files\n",
    "csv_path = 'data/sample.csv'  # Update with your file path\n",
    "parquet_path = 'data/sample.parquet'\n",
    "json_path = 'data/sample.json'\n",
    "\n",
    "# Read CSV\n",
    "df_csv = spark.read.option('header', True).csv(csv_path)\n",
    "df_csv.show()\n",
    "\n",
    "# Read Parquet\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "df_parquet.show()\n",
    "\n",
    "# Read JSON\n",
    "df_json = spark.read.json(json_path)\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformations & Actions\n",
    "\n",
    "Learn the difference between transformations (lazy) and actions (trigger execution) in Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations: filter, select, withColumn\n",
    "df_filtered = df.filter(df.age > 25)\n",
    "df_selected = df.select('name', 'age')\n",
    "df_newcol = df.withColumn('age_plus_10', df.age + 10)\n",
    "\n",
    "# Actions: show, count, collect\n",
    "df_filtered.show()\n",
    "print('Count:', df.count())\n",
    "print('Names:', df_selected.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ETL Pipeline Example\n",
    "\n",
    "A simple ETL pipeline: Extract data, Transform it, and Load it to storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ETL Pipeline Example\n",
    "## Extract\n",
    "input_path = 'data/input.csv'  # Update with your file path\n",
    "df_etl = spark.read.option('header', True).csv(input_path)\n",
    "\n",
    "## Transform\n",
    "df_etl_clean = df_etl.dropna().withColumnRenamed('old_column', 'new_column')\n",
    "\n",
    "## Load\n",
    "output_path = 'data/output.parquet'\n",
    "df_etl_clean.write.mode('overwrite').parquet(output_path)\n",
    "print('ETL pipeline completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DataFrame Joins & Aggregations\n",
    "\n",
    "Practice joining DataFrames and performing aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Joins Example\n",
    "df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'val1'])\n",
    "df2 = spark.createDataFrame([(1, 'X'), (2, 'Y')], ['id', 'val2'])\n",
    "\n",
    "df_joined = df1.join(df2, on='id', how='inner')\n",
    "df_joined.show()\n",
    "\n",
    "# Aggregation Example\n",
    "from pyspark.sql import functions as F\n",
    "df_agg = df.groupBy('age').agg(F.count('id').alias('count'))\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Writing Data (Parquet, Delta, etc.)\n",
    "\n",
    "Save DataFrames to different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet\n",
    "df.write.mode('overwrite').parquet('data/output_df.parquet')\n",
    "\n",
    "# If using Delta Lake (Databricks), you can write as Delta format:\n",
    "# df.write.format('delta').mode('overwrite').save('data/output_df_delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Useful Tips & Resources\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- Use `.explain()` to understand query plans.\n",
    "- Use `.cache()` for performance when reusing DataFrames.\n",
    "\n",
    "Happy Practising! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Useful Tips & Resources\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- Use `.explain()` to understand query plans.\n",
    "- Use `.cache()` for performance when reusing DataFrames.\n",
    "\n",
    "Happy Practising! ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet\n",
    "df.write.mode('overwrite').parquet('data/output_df.parquet')\n",
    "\n",
    "# If using Delta Lake (Databricks), you can write as Delta format:\n",
    "# df.write.format('delta').mode('overwrite').save('data/output_df_delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Writing Data (Parquet, Delta, etc.)\n",
    "\n",
    "Save DataFrames to different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Joins Example\n",
    "df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'val1'])\n",
    "df2 = spark.createDataFrame([(1, 'X'), (2, 'Y')], ['id', 'val2'])\n",
    "\n",
    "df_joined = df1.join(df2, on='id', how='inner')\n",
    "df_joined.show()\n",
    "\n",
    "# Aggregation Example\n",
    "from pyspark.sql import functions as F\n",
    "df_agg = df.groupBy('age').agg(F.count('id').alias('count'))\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DataFrame Joins & Aggregations\n",
    "\n",
    "Practice joining DataFrames and performing aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ETL Pipeline Example\n",
    "## Extract\n",
    "input_path = 'data/input.csv'  # Update with your file path\n",
    "df_etl = spark.read.option('header', True).csv(input_path)\n",
    "\n",
    "## Transform\n",
    "df_etl_clean = df_etl.dropna().withColumnRenamed('old_column', 'new_column')\n",
    "\n",
    "## Load\n",
    "output_path = 'data/output.parquet'\n",
    "df_etl_clean.write.mode('overwrite').parquet(output_path)\n",
    "print('ETL pipeline completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ETL Pipeline Example\n",
    "\n",
    "A simple ETL pipeline: Extract data, Transform it, and Load it to storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations: filter, select, withColumn\n",
    "df_filtered = df.filter(df.age > 25)\n",
    "df_selected = df.select('name', 'age')\n",
    "df_newcol = df.withColumn('age_plus_10', df.age + 10)\n",
    "\n",
    "# Actions: show, count, collect\n",
    "df_filtered.show()\n",
    "print('Count:', df.count())\n",
    "print('Names:', df_selected.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformations & Actions\n",
    "\n",
    "Learn the difference between transformations (lazy) and actions (trigger execution) in Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading CSV, Parquet, and JSON files\n",
    "csv_path = 'data/sample.csv'  # Update with your file path\n",
    "parquet_path = 'data/sample.parquet'\n",
    "json_path = 'data/sample.json'\n",
    "\n",
    "# Read CSV\n",
    "df_csv = spark.read.option('header', True).csv(csv_path)\n",
    "df_csv.show()\n",
    "\n",
    "# Read Parquet\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "df_parquet.show()\n",
    "\n",
    "# Read JSON\n",
    "df_json = spark.read.json(json_path)\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Ingestion (CSV, Parquet, JSON)\n",
    "\n",
    "Read data from different file formats into DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1, 'Alice', 29), (2, 'Bob', 31), (3, 'Cathy', 25)]\n",
    "columns = ['id', 'name', 'age']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# Select columns\n",
    "df.select('name', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "Create a DataFrame, view schema, and perform simple operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName('PySparkPractice').getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print('Spark Version:', spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SparkSession Initialization\n",
    "\n",
    "Create a SparkSession, which is the entry point to using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark if not already installed\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Installation\n",
    "\n",
    "Ensure you have PySpark installed. If not, run the following cell to install it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "openjdk version \"17.0.15\" 2025-04-15\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.15+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.15+0, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
