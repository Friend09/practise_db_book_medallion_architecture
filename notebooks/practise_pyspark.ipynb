{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTISE\n",
    "\n",
    "# PySpark Practice Notebook for Data Engineering & ETL on Databricks\n",
    "\n",
    "This notebook is designed for hands-on practice with PySpark, focusing on ETL pipelines and data engineering tasks commonly performed in Databricks environments.\n",
    "\n",
    "## Sections:\n",
    "\n",
    "1. Environment Setup & Installation\n",
    "2. SparkSession Initialization\n",
    "3. Basic DataFrame Operations\n",
    "4. Data Ingestion (CSV, Parquet, JSON)\n",
    "5. Transformations & Actions\n",
    "6. ETL Pipeline Example\n",
    "7. DataFrame Joins & Aggregations\n",
    "8. Writing Data (Parquet, Delta, etc.)\n",
    "9. Useful Tips & Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Installation\n",
    "\n",
    "Ensure you have PySpark installed. If not, run the following cell to install it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install PySpark if not already installed\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SparkSession Initialization\n",
    "\n",
    "Create a SparkSession, which is the entry point to using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize SparkSession\n",
    "# spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n",
    "\n",
    "# # Check Spark version\n",
    "# print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/11 20:34:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/11 20:35:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "Create a DataFrame, view schema, and perform simple operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a sample DataFrame\n",
    "# data = [(1, \"Alice\", 29), (2, \"Bob\", 31), (3, \"Cathy\", 25)]\n",
    "# columns = [\"id\", \"name\", \"age\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# # Show DataFrame\n",
    "# df.show()\n",
    "\n",
    "# # Print schema\n",
    "# df.printSchema()\n",
    "\n",
    "# # Select columns\n",
    "# df.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 29|\n",
      "|  2|  Bob| 31|\n",
      "|  3|Cathy| 25|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 31), (3, \"Cathy\", 25)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Ingestion (CSV, Parquet, JSON)\n",
    "\n",
    "Read data from different file formats into DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Reading CSV, Parquet, and JSON files\n",
    "# csv_path = '../data/sample.csv'  # Update with your file path\n",
    "# parquet_path = '../data/sample.parquet'\n",
    "# json_path = '../data/sample.json'\n",
    "\n",
    "# # Read CSV\n",
    "# df_csv = spark.read.option('header', True).csv(csv_path)\n",
    "# df_csv.show()\n",
    "\n",
    "# # Read Parquet\n",
    "# df_parquet = spark.read.parquet(parquet_path)\n",
    "# df_parquet.show()\n",
    "\n",
    "# # Read JSON\n",
    "# df_json = spark.read.json(json_path)\n",
    "# df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|age| id| name|\n",
      "+---+---+-----+\n",
      "| 29|  1|Alice|\n",
      "| 31|  2|  Bob|\n",
      "| 25|  3|Cathy|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"../data/sample.csv\"\n",
    "parquet_path = \"../data/sample.parquet\"\n",
    "json_path = \"../data/sample.json\"\n",
    "\n",
    "df_csv = spark.read.option(\"header\", True).csv(csv_path)\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "df_json = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformations & Actions\n",
    "\n",
    "Learn the difference between transformations (lazy) and actions (trigger execution) in Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations: filter, select, withColumn\n",
    "df_filtered = df.filter(df.age > 25)\n",
    "df_selected = df.select('name', 'age')\n",
    "df_newcol = df.withColumn('age_plus_10', df.age + 10)\n",
    "\n",
    "# Actions: show, count, collect\n",
    "df_filtered.show()\n",
    "print('Count:', df.count())\n",
    "print('Names:', df_selected.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 29|\n",
      "|  2|  Bob| 31|\n",
      "|  3|Cathy| 25|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 29|\n",
      "|  2|  Bob| 31|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age>25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 29|\n",
      "|  Bob| 31|\n",
      "|Cathy| 25|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=29), Row(name='Bob', age=31), Row(name='Cathy', age=25)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected = df.select(\"name\", \"age\")\n",
    "df_selected.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----------+\n",
      "| id| name|age|age_plus_10|\n",
      "+---+-----+---+-----------+\n",
      "|  1|Alice| 29|         39|\n",
      "|  2|  Bob| 31|         41|\n",
      "|  3|Cathy| 25|         35|\n",
      "+---+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('age_plus_10', df.age + 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ETL Pipeline Example\n",
    "\n",
    "A simple ETL pipeline: Extract data, Transform it, and Load it to storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ETL Pipeline Example\n",
    "## Extract\n",
    "input_path = '../data/input.csv'  # Update with your file path\n",
    "df_etl = spark.read.option('header', True).csv(input_path)\n",
    "\n",
    "## Transform\n",
    "df_etl_clean = df_etl.dropna().withColumnRenamed('old_column', 'new_column')\n",
    "\n",
    "## Load\n",
    "output_path = '../data/output.parquet'\n",
    "df_etl_clean.write.mode('overwrite').parquet(output_path)\n",
    "print('ETL pipeline completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 29|\n",
      "|  2|  Bob| 31|\n",
      "|  3|Cathy| 25|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../data/input.csv\"\n",
    "df_etl = spark.read.option(\"header\", True).csv(input_path)\n",
    "\n",
    "df_etl_clean = df_etl.dropna().withColumnRenamed(\"old_column\", \"new_column\")\n",
    "df_etl_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL pipeline completed\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../data/output.parquet\"\n",
    "df_etl_clean.write.mode('overwrite').parquet(output_path)\n",
    "print('ETL pipeline completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DataFrame Joins & Aggregations\n",
    "\n",
    "Practice joining DataFrames and performing aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Joins Example\n",
    "df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'val1'])\n",
    "df2 = spark.createDataFrame([(1, 'X'), (2, 'Y')], ['id', 'val2'])\n",
    "\n",
    "df_joined = df1.join(df2, on='id', how='inner')\n",
    "df_joined.show()\n",
    "\n",
    "# Aggregation Example\n",
    "from pyspark.sql import functions as F\n",
    "df_agg = df.groupBy('age').agg(F.count('id').alias('count'))\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|val1|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "| id|val2|\n",
      "+---+----+\n",
      "|  1|   X|\n",
      "|  2|   Y|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'A'), (2, 'B')], ['id', 'val1'])\n",
    "df2 = spark.createDataFrame([(1,'X'), (2, 'Y')], ['id', 'val2'])\n",
    "\n",
    "df1.show(), df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 29|    1|\n",
      "| 31|    1|\n",
      "| 25|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(\n",
    "    df\n",
    "    .groupBy('age')\n",
    "    .agg(F.count('id').alias('count'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Writing Data (Parquet, Delta, etc.)\n",
    "\n",
    "Save DataFrames to different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet\n",
    "df.write.mode('overwrite').parquet('..data/output_df.parquet')\n",
    "\n",
    "# If using Delta Lake (Databricks), you can write as Delta format:\n",
    "# df.write.format('delta').mode('overwrite').save('data/output_df_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet('/Users/vamsi_mbmax/Developer/VAM_Documents/01_vam_PROJECTS/LEARNING/proj_Databases/dev_proj_Databases/practise_db_book_medallion_architecture/data/output_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Useful Tips & Resources\n",
    "\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- Use `.explain()` to understand query plans.\n",
    "- Use `.cache()` for performance when reusing DataFrames.\n",
    "\n",
    "Happy Practising! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practise_db_book_medallion_architecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
